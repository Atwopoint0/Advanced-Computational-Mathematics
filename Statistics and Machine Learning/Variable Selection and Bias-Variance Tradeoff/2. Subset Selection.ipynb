{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOgqUib8PO4IL/XvH+MJlU2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We consider two approaches to variable selection, subset selection and shrinkage-based methods. Subset selection methods look among all possible subsets of variables for the one that minimises some suitable estimate of prediction error. The search problem becomes infeasible for large $p$, and non-exhaustive greedy search methods have to be employed.\n","\n","Shrinkage-based variable selection methods will instead penalise the RSS by a penalty term that forces the LS regression coefficients to shrink in a manner that favours exact zeros in $\\hat\\beta$.\n","\n","The best subset method takes as input a training dataset $\\mathcal{T}$ and outputs a $p \\times p$ matrix $B$, whose $j$-th column contains $\\hat\\beta^{\\mathcal{M}_j}$ for the best performing model (in the sense of RSS) of size $j$, $\\mathcal{M}_j$:\n","\\begin{equation}\n","    \\mathcal{M}_j(\\mathcal{T}) = \\underset{\\mathcal{M} : \\|M\\| = j}{\\operatorname{argmin}} RSS(\\hat\\beta^{\\mathcal{M}}(\\mathcal{T}); \\mathcal{T}).\n","\\end{equation}\n"],"metadata":{"id":"JpmMRuVo2xGi"}},{"cell_type":"code","source":["import numpy as np\n","import itertools\n","\n","def best_subset(X, y):\n","    '''\n","    Computes the best subset selection for linear regression.\n","    Args:\n","        X: N x p matrix of covariates (standardised/centered).\n","        y: N-dimensional vector of responses.\n","    Returns:\n","        B: p x p matrix. The j-th column (index j-1) contains\n","           the coefficients of the best model of size j.\n","    '''\n","    N, p = X.shape\n","    B = np.zeros((p, p))\n","\n","    # Iterate over every possible model size j from 1 to p\n","    for j in range(1, p + 1):\n","        best_rss = np.inf\n","        best_beta_subset = None\n","        best_indices = None\n","\n","        # Iterate over all unique combinations of columns of size j\n","        # combinations returns tuples of indices like (0, 1, 4)\n","        for indices in itertools.combinations(range(p), j):\n","            X_subset = X[:, indices]\n","\n","            # Fit OLS: beta = (X'X)^-1 X'y\n","            beta_subset, residues, rank, s = np.linalg.lstsq(X_subset, y, rcond=None)\n","\n","            # Calculate RSS\n","            y_pred = X_subset @ beta_subset\n","            rss = np.sum((y - y_pred)**2)\n","\n","            # Update best model if this subset has lower RSS\n","            if rss < best_rss:\n","                best_rss = rss\n","                best_beta_subset = beta_subset\n","                best_indices = indices\n","\n","        # Store the best coefficients in the j-th column of B\n","        # Map subset coefficients back to their original positions\n","        if best_indices is not None:\n","            B[list(best_indices), j-1] = best_beta_subset\n","\n","    return B"],"metadata":{"id":"w0KGJt2L2_dB","executionInfo":{"status":"ok","timestamp":1766437240096,"user_tz":0,"elapsed":9,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["The model space is the set of all possible subsets of the covariates $\\{1, \\dots, p\\}$. The size of the model space is $|\\mathcal{M}| = 2^p$ (including the empty set). Typically, we need to find the smallest integer $p$ such that the number of subsets for a specific size $j$, given by the binomial coefficient $\\binom{p}{j}$, exceeds a given upper bound $C$,\n","\\begin{equation}\n","    \\binom{p}{j} > C \\quad \\text{for some } j \\in \\{1, \\dots, p\\}.\n","\\end{equation}\n","The binomial coefficient is maximised when $j$ is in the middle of the range, i.e., $j \\approx p/2$. This gives the feasible range for this method.\n","\n","---\n","\n","The method greedy subset performs the same proceedure but it incrementally builds up the model sequence $\\mathcal{M}_j$ by adding at each iteration the covariate that improves model fit the most\n","\\begin{equation}\n","    \\mathcal{M}_0 = \\emptyset, \\quad \\mathcal{M}_{d+1}(\\mathcal{T}) = \\mathcal{M}_d(\\mathcal{T}) \\cup \\left\\{ l | l = \\underset{j}{\\operatorname{argmin}} RSS (\\hat\\beta^{\\mathcal{M}_d(\\mathcal{T}) \\cup \\{j\\}}(\\mathcal{T}); \\mathcal{T})\\right\\}.\n","\\end{equation}"],"metadata":{"id":"_5oVTODS3U2r"}},{"cell_type":"code","source":["def greedysubset(X, y):\n","    '''\n","    Computes the greedy subset selection (forward stepwise) for linear regression.\n","    Args:\n","        X: N x p matrix of covariates.\n","        y N-dimensional vector of responses.\n","    Returns:\n","        B: p x p matrix. The j-th column contains\n","           the coefficients of the greedy model of size j.\n","    '''\n","    N, p = X.shape\n","    B = np.zeros((p, p))\n","\n","    # Track selected and remaining feature indices\n","    selected_indices = []\n","    remaining_indices = list(range(p))\n","\n","    # Iteratively build models of size 1 to p\n","    for size in range(1, p + 1):\n","        best_rss = np.inf\n","        best_candidate = -1\n","        best_beta_subset = None\n","\n","        # Iterate through all remaining candidates to find the best addition\n","        for candidate in remaining_indices:\n","            # Trial subset: currently selected + candidate\n","            trial_indices = selected_indices + [candidate]\n","            X_subset = X[:, trial_indices]\n","\n","            # Fit OLS\n","            beta_subset, _, _, _ = np.linalg.lstsq(X_subset, y, rcond=None)\n","\n","            # Compute RSS\n","            y_pred = X_subset @ beta_subset\n","            rss = np.sum((y - y_pred)**2)\n","\n","            # Check if this is the best improvement\n","            if rss < best_rss:\n","                best_rss = rss\n","                best_candidate = candidate\n","                best_beta_subset = beta_subset\n","\n","        # Permanently add the best candidate to the model\n","        selected_indices.append(best_candidate)\n","        remaining_indices.remove(best_candidate)\n","\n","        # Store coefficients in the output matrix B\n","        # Map subset coeffs to correct row indices.\n","        B[selected_indices, size - 1] = best_beta_subset\n","\n","    return B"],"metadata":{"id":"xzjnnZhF6SyB","executionInfo":{"status":"ok","timestamp":1766438173647,"user_tz":0,"elapsed":5,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["The nested property of the models $\\mathcal{M}_0 \\subset \\mathcal{M}_1 \\subset \\dots \\subset \\mathcal{M}_p$ can be exploited for computational efficiency. Instead of solving the LS problem from scratch for every step (which involves inverting a matrix or solving a system of equations at a cost of $O(j^3)$), we can update the inverse of the Gram matrix $(X^T X)^{-1}$ from the previous step.\n","\n","Let $X_j$ be the design matrix for the model with $j$ variables, and let $(X_j^T X_j)^{-1}$ be known. When we move to model $\\mathcal{M}_{j+1}$, we append a new column vector $x$ to $X_j$, creating $X_{j+1} = \\begin{pmatrix} X_j & x \\end{pmatrix}$. The new Gram matrix is a block matrix:\n","\\begin{equation}\n","    X_{j+1}^T X_{j+1} =\n","    \\begin{pmatrix}\n","        X_j^T X_j & X_j^T x \\\\\n","        x^T X_j & x^T x\n","    \\end{pmatrix} =\n","    \\begin{pmatrix}\n","        A & b \\\\\n","        b^T & c\n","    \\end{pmatrix}\n","\\end{equation}\n","where $A = X_j^T X_j$. Using the block matrix inversion formula, the inverse is\n","\\begin{equation}\n","    (X_{j+1}^T X_{j+1})^{-1} =\n","    \\begin{pmatrix}\n","        A^{-1} + \\frac{1}{k} (A^{-1}b)(A^{-1}b)^T & -\\frac{1}{k} A^{-1}b \\\\\n","        -\\frac{1}{k} b^T A^{-1} & \\frac{1}{k}\n","    \\end{pmatrix}\n","\\end{equation}\n","where $k = c - b^T A^{-1} b$ is the Schur complement. The upper left $j \\times j$ block of the new inverse is $A^{-1} + \\frac{1}{k} (A^{-1}b)(A^{-1}b)^T$ which can be computed from the old inverse by adding a rank-$1$ update matrix. This update only relies on matrix-vector multiplications which only costs $O(j^2)$.\n","\n","---\n","\n","We can amend the greedy subset method to stop whenever the newly added variable does not significantly improve fit, using the $F$-statistic\n","\\begin{equation}\n","    \\frac{RSS(\\hat\\beta^{\\mathcal{M}_d}) - RSS(\\hat\\beta^{\\mathcal{M}_{d+1}})}{RSS(\\hat\\beta^{\\mathcal{M}_{d+1}})/(N - d - 1)},\n","\\end{equation}\n","which follows an $F_{1,N-d-1}$ distribution."],"metadata":{"id":"aONGFFMQ6-C_"}},{"cell_type":"code","source":["from scipy.stats import f\n","\n","def forward_f_test_subset(X, y, alpha=0.05):\n","    '''\n","    Performs forward stepwise selection with an F-test stopping criterion.\n","    Args:\n","        X: N x p matrix of covariates.\n","        y: N-dimensional vector of responses.\n","        alpha: Significance level.\n","    Returns:\n","        model_indices: List of indices of the selected covariates.\n","    '''\n","    N, p = X.shape\n","    selected_indices = []\n","    remaining_indices = list(range(p))\n","\n","    # Calculate RSS for the initial empty model (M_0)\n","    # Since variables are zero-mean, the intercept-only prediction is 0.\n","    # RSS_0 = sum((y - 0)^2) / N = mean(y^2)\n","    current_rss = np.mean(y**2)\n","\n","    # We iterate to potentially add p variables\n","    for d in range(p):\n","        best_rss = np.inf\n","        best_candidate = -1\n","        best_beta = None\n","\n","        # Greedy Step: Find the best variable to add\n","        for candidate in remaining_indices:\n","            trial_indices = selected_indices + [candidate]\n","            X_subset = X[:, trial_indices]\n","\n","            # Fit OLS\n","            beta_subset, _, _, _ = np.linalg.lstsq(X_subset, y, rcond=None)\n","\n","            # Compute RSS (Mean Squared Error formulation)\n","            y_pred = X_subset @ beta_subset\n","            rss_trial = np.mean((y - y_pred)**2)\n","\n","            if rss_trial < best_rss:\n","                best_rss = rss_trial\n","                best_candidate = candidate\n","\n","        # F-Test Step: Compare M_d with M_{d+1}\n","        # F = (RSS_d - RSS_{d+1}) / (RSS_{d+1} / (N - d - 1))\n","\n","        numerator = current_rss - best_rss\n","        denominator = best_rss / (N - d - 1)\n","\n","        # Avoid division by zero if perfect fit\n","        if denominator == 0:\n","            F_stat = np.inf\n","        else:\n","            F_stat = numerator / denominator\n","\n","        # Compute p-value using F distribution with df1=1, df2=N-d-1\n","        p_value = 1 - f.cdf(F_stat, 1, N - d - 1)\n","\n","        # Stopping Criterion\n","        if p_value > alpha:\n","            print(f\"Stopping at size {d}. Best candidate (idx {best_candidate}) \"\n","                  f\"p-value: {p_value:.4f} > {alpha}\")\n","            break\n","\n","        # If significant, update model and continue\n","        selected_indices.append(best_candidate)\n","        remaining_indices.remove(best_candidate)\n","        current_rss = best_rss\n","\n","    return selected_indices"],"metadata":{"id":"TFqRoPng-IC9","executionInfo":{"status":"ok","timestamp":1766439115750,"user_tz":0,"elapsed":1068,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["This method would not work for best subset selection since the $F$-test statistic is designed for nested hypothesis testing. It measures the statistical significance of the reduction in error variance when adding parameters to an existing model.\n","\n","Furthermore, even if the models happened to be nested, applying an $F$-test to the best subset found introduces a selection bias. The $F$-statistic assumes that the model structure was chosen a priori. When we select the variable that maximises the reduction in RSS from a set of $\\binom{p}{d}$ combinations, the resulting $F$-statistic will be inflated. The distribution of this maximum statistic is not the standard $F_{1, N-d-1}$, meaning the $p$-values would be too small, leading to the inclusion of false positives.\n","\n","---\n","\n","We can represent a sparse (linear regression) estimator more generally as\n","an algorithm that takes as input a training set $\\mathcal{T}$ and outputs a sequence of $p$ candidate regression vectors for each model size (i.e., the $j$-th candidate $\\hat\\beta(j)(\\mathcal{T})$ has precisely $p - j$ zeros). Best and greedy subset search are special cases of this definition for which each\n","candidate is a least squares solution, a condition we will not insist on here.\n","\n","We would like to select among candidates on the basis of estimated prediction error $\\hat{PE}$:\n","\\begin{equation}\n","    \\hat\\beta^{CV}(\\mathcal{T}) = \\hat\\beta^{j^*}(\\mathcal{T}), \\quad \\text{where} j^* = \\underset{j}{\\operatorname{argmin}} \\{\\hat{PE}(j, \\mathcal{T})\\}.\n","\\end{equation}\n","The prediction error can be estimated using $10$-fold cross-validation as\n","\\begin{equation}\n","    \\hat{PE}(j, \\mathcal{T}) = \\frac{1}{10}\\sum_{k=1}^{10} RSS(\\hat\\beta^{(j)}(\\mathcal{T}^{-k}); \\mathcal{T}^k),\n","\\end{equation}\n","where $\\mathcal{T}^k$ is the $k$-th fold of the training set and $\\mathcal{T}^{-k}$ its complement\n","\\begin{align}\n","    \\mathcal{T}^k &= \\left\\{ (y_{\\pi(n)}, x_{\\pi(n)}) | k - 1 < \\frac{10n}{N} \\leq k \\right\\}, \\\\\n","    \\mathcal{T}^{-k} &= \\left\\{(y_{\\pi(n)}, x_{\\pi(n)}) | \\frac{10n}{N} \\leq k âˆ’ 1 \\text{ or } \\frac{10n}{N} > k \\right\\},\n","\\end{align}\n","where $\\pi$ is a random permutation of $\\{1, \\dots, N\\}$.\n"],"metadata":{"id":"Tprc9ihp-m-k"}},{"cell_type":"code","source":["def crossval(X, y, sparse_estimator):\n","    '''\n","    Performs 10-fold cross-validation to select the best model size.\n","    Args:\n","        X: N x p matrix of covariates.\n","        y: N-dimensional vector of responses.\n","        sparse_estimator: A function that takes (X, y) and returns a p x p matrix\n","                          of coefficients (columns correspond to model sizes 1 to p).\n","    Returns:\n","        beta_cv: The coefficient vector of the best performing model.\n","    '''\n","    N, p = X.shape\n","    K = 10\n","\n","    # Create random permutation of indices\n","    indices = np.random.permutation(N)\n","\n","    # Split indices into 10 roughly equal folds\n","    folds = np.array_split(indices, K)\n","\n","    # Matrix to store RSS for each fold (rows) and each model size (cols)\n","    rss_matrix = np.zeros((K, p))\n","\n","    # Iterate through each fold\n","    for k in range(K):\n","        # The k-th fold is the validation set\n","        test_idx = folds[k]\n","\n","        # The complement is the training set\n","        mask = np.ones(N, dtype=bool)\n","        mask[test_idx] = False\n","        train_idx = np.where(mask)[0]\n","\n","        X_tr, y_tr = X[train_idx], y[train_idx]\n","        X_te, y_te = X[test_idx], y[test_idx]\n","\n","        # Compute candidate models on training Set\n","        # B_matrix is p x p, where col j corresponds to model size j+1\n","        B_matrix = sparse_estimator(X_tr, y_tr)\n","\n","        # Evaluate prediction error on validation set for all sizes\n","        for j in range(p):\n","            # Coefficients for model size j+1\n","            beta_j = B_matrix[:, j]\n","            # Prediction\n","            y_pred = X_te @ beta_j\n","            # Compute RSS\n","            rss_matrix[k, j] = np.sum((y_te - y_pred)**2)\n","\n","    # Select best model size\n","    # Calculate mean RSS across the 10 folds for each model size\n","    mean_rss = np.mean(rss_matrix, axis=0)\n","\n","    # Find the index minimising the error (j*)\n","    best_j_index = np.argmin(mean_rss)\n","\n","    # Refit on full dataset\n","    # Return the estimator trained on complete dataset T selected at optimal size j*.\n","    B_final = sparse_estimator(X, y)\n","    beta_cv = B_final[:, best_j_index]\n","\n","    return beta_cv"],"metadata":{"id":"F2-XnsTWFQQH","executionInfo":{"status":"ok","timestamp":1766441062067,"user_tz":0,"elapsed":52,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}}},"execution_count":4,"outputs":[]}]}
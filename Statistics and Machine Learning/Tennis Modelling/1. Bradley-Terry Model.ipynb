{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNY5vxRohfs6h7rhklOZ56O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The Bradley-Terry model for a tournament considers each match to be independent, and\n","\\begin{equation}\n","    \\Pr(\\text{Player $a$ wins a match against Player $b$}) = \\frac{\\exp(\\beta_a - \\beta_b)}{1 + exp(\\beta_a - \\beta_b)},\n","\\end{equation}\n","for a vector of parameters $\\beta$ of equal length as the number of players. This model was first invented as a way to rank chess players.\n","\n","The response variable is the outcome of a specific match. Since there are only two possible outcomes for each match, and the matches are considered independent, the response follows a Bernoulli distribution. Denote $\\pi = \\Pr(a \\text{ wins})$ and the linear predictor $\\eta = \\beta_a - \\beta_b$, we can rearrange the equation to obtain\n","\\begin{equation}\n","    \\log\\left(\\frac{\\pi}{1 - \\pi}\\right) = \\eta.\n","\\end{equation}\n","Therefore, the link function connecting the mean response to the linear predictor is the logit link function\n","\\begin{equation}\n","    g(\\pi) = \\operatorname{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1 - \\pi}\\right).\n","\\end{equation}\n","Let $N$ be the number of matches and $p$ be the number of unique players. The parameter vector $\\beta$ has length $p$. The linear predictor for a match between Player $a$ and Player $b$ is $\\eta = \\beta_a - \\beta_b$. The design matrix $X$ is an $N \\times p$ matrix where for a specific row representing a match where Player $a$ is the first player and Player $b$ is the second player:\n","\n","*   The column corresponding to Player $a$ takes the value $1$.\n","*   The column corresponding to Player $b$ takes the value $-1$.\n","*   All other columns are $0$.\n","\n","Usually a constraint, such as $\\beta_1 = 0$ or $\\sum \\beta_i = 0$, is applied as the above design matrix is rank-deficient by $1$. If we exchange the order of players, then we are calculating the probability that Player $b$ wins against Player $a$. Here, the linear predictor changes sign and in the probabiliy fornula, $\\Pr( b \\text{ wins}) = 1 - \\Pr(a \\text{ wins})$."],"metadata":{"id":"Bma5_whSlENs"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import statsmodels.api as sm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Advanced Computational Maths/Statistics and Machine Learning/Tennis Modelling'\n","df = pd.read_csv(path + '/mensResults.csv', encoding='latin1')\n","df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4i1OQASsdRt","executionInfo":{"status":"ok","timestamp":1766625749547,"user_tz":0,"elapsed":1050,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"1e874d79-e483-444a-c678-cd16c50bcad3"},"execution_count":179,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["def preprocess_data(df):\n","    '''\n","    Converts dates and splits data into training (2000-2014) and testing (2015-2016) sets.\n","    '''\n","    train_mask = (df['Date'].dt.year >= 2000) & (df['Date'].dt.year <= 2014)\n","    test_mask = (df['Date'].dt.year >= 2015) & (df['Date'].dt.year <= 2016)\n","\n","    return df.loc[train_mask].copy(), df.loc[test_mask].copy()\n","\n","def create_player_map(df, ref_player):\n","    '''\n","    Identifies all unique players in the training set and creates a dictionary\n","    mapping player names to column indices. The reference player is excluded\n","    from the map (coefficient fixed at 0).\n","    '''\n","    # Get all unique players\n","    unique_players = set(df['Winner'].unique()) | set(df['Loser'].unique())\n","\n","    # Remove reference player so they don't get a column in X\n","    if ref_player in unique_players:\n","        unique_players.remove(ref_player)\n","\n","    # Sort for consistency and create map\n","    sorted_players = sorted(list(unique_players))\n","    return {player: i for i, player in enumerate(sorted_players)}\n","\n","def build_design_matrix(df, player_map, ref_player):\n","    '''\n","    Constructs the design matrix X and response vector y.\n","    X entries: +1 for Winner, -1 for Loser, 0 otherwise.\n","    y entries: Always 1 (representing that the 'Winner' won).\n","\n","    Filters out matches involving players not present in player_map\n","    (unless they are the reference player).\n","    '''\n","    N_matches = len(df)\n","    N_features = len(player_map)\n","\n","    winners = df['Winner'].values\n","    losers = df['Loser'].values\n","\n","    valid_indices = []\n","    X_list = []\n","\n","    for i in range(N_matches):\n","        w, l = winners[i], losers[i]\n","\n","        # Check if players are \"known\" (in the map OR are the reference player)\n","        w_is_ref = (w == ref_player)\n","        l_is_ref = (l == ref_player)\n","        w_in_map = (w in player_map)\n","        l_in_map = (l in player_map)\n","\n","        # Proceed only if we can account for both players\n","        if (w_in_map or w_is_ref) and (l_in_map or l_is_ref):\n","            row = np.zeros(N_features)\n","\n","            if w_in_map:\n","                row[player_map[w]] = 1\n","            if l_in_map:\n","                row[player_map[l]] = -1\n","\n","            X_list.append(row)\n","            valid_indices.append(i)\n","\n","    X = np.array(X_list)\n","    y = np.ones(len(X)) # Response is always 1\n","\n","    return X, y\n","\n","def train_bradley_terry(X_train, y_train):\n","    '''\n","    Fits the Bradley-Terry model using Logistic Regression without an intercept.\n","    '''\n","    # sm.Logit does not add an intercept by default.\n","    model = sm.Logit(y_train, X_train)\n","    result = model.fit(method='ncg', disp=0)\n","    return result\n","\n","def calculate_logistic_loss(result, X, y):\n","    '''\n","    Calculates the average negative log-likelihood (Logistic Loss).\n","    Formula: - (1/N) * sum( log(predicted_probability) )\n","    '''\n","    # Predict probability that y=1 (Winner wins)\n","    preds = result.predict(X)\n","\n","    # Clip probabilities slightly to prevent log(0) error\n","    epsilon = 1e-15\n","    preds = np.clip(preds, epsilon, 1 - epsilon)\n","\n","    # Calculate Mean Negative Log Likelihood\n","    loss = -np.mean(np.log(preds))\n","    return loss"],"metadata":{"id":"B-J5bK6CsUln","executionInfo":{"status":"ok","timestamp":1766625749667,"user_tz":0,"elapsed":113,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}}},"execution_count":180,"outputs":[]},{"cell_type":"code","source":["df_train, df_test = preprocess_data(df)\n","\n","print(f\"Training Samples: {len(df_train)}, Test Samples: {len(df_test)}\")\n","REF_PLAYER = \"Agassi A.\"\n","player_map = create_player_map(df_train, REF_PLAYER)\n","print(f\"Number of predictors (players excluding ref): {len(player_map)}\")\n","\n","X_train, y_train = build_design_matrix(df_train, player_map, REF_PLAYER)\n","X_test, y_test = build_design_matrix(df_test, player_map, REF_PLAYER)\n","model_result = train_bradley_terry(X_train, y_train)\n","\n","train_loss = calculate_logistic_loss(model_result, X_train, y_train)\n","test_loss = calculate_logistic_loss(model_result, X_test, y_test)\n","print(f\"\\nLogistic Loss (Train 2000-2014): {train_loss:.5f}\")\n","print(f\"Logistic Loss (Test 2015-2016):  {test_loss:.5f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2BLvjdEvr2N","executionInfo":{"status":"ok","timestamp":1766625751169,"user_tz":0,"elapsed":1499,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"bfaf9a40-d307-4f0f-8be3-b99f744c8248"},"execution_count":181,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Samples: 4489, Test Samples: 420\n","Number of predictors (players excluding ref): 59\n","\n","Logistic Loss (Train 2000-2014): 0.59937\n","Logistic Loss (Test 2015-2016):  0.55010\n"]}]},{"cell_type":"markdown","source":["The choice of reference player does not affect the fitted values (the predicted probabilities of winning). In the Bradley-Terry model, the probability that Player $i$ beats Player $j$ depends only on the difference between their coefficients $\\beta_i - \\beta_j$. Choosing a different reference player effectively shifts all coefficients by a constant constant $c$ so the difference remains unchanged."],"metadata":{"id":"5QHNmGsVw9lC"}},{"cell_type":"markdown","source":["To find the confidence interval for a probability in a logistic regression model, we calculate the CI for the linear predictor first and then transform the endpoints using the logistic function. This approach is preferred because the distribution of the linear predictor is closer to normal than the distribution of the probability itself.\n","\n","The linear predictor for Federer beating Murray is:\n","\\begin{equation}\n","    \\eta = \\hat{\\beta}_{\\text{Federer}} - \\hat{\\beta}_{\\text{Murray}}.\n","\\end{equation}\n","Since $\\hat{\\beta}_{\\text{Federer}}$ and $\\hat{\\beta}_{\\text{Murray}}$ are correlated estimates from the same model, the variance of their difference is\n","\\begin{equation}\n","    \\text{Var}(\\eta) = \\text{Var}(\\hat{\\beta}_F) + \\text{Var}(\\hat{\\beta}_M) - 2\\text{Cov}(\\hat{\\beta}_F, \\hat{\\beta}_M).\n","\\end{equation}\n","A $68\\%$ confidence interval corresponds roughly to one standard error ($Z$-score $\\approx 1$) on either side of the mean, assuming a normal approximation.\n","\\begin{equation}\n","    SE(\\eta) = \\sqrt{\\text{Var}(\\eta)}, \\quad \\text{CI}_{\\eta} = [\\eta - SE(\\eta), \\eta + SE(\\eta)].\n","\\end{equation}\n","Finally, apply the logistic inverse link function to the endpoints\n","\\begin{equation}\n","    \\Pr(\\text{Federer wins}) = \\frac{e^{\\eta}}{1 + e^{\\eta}}.\n","\\end{equation}"],"metadata":{"id":"cR4yhMiiyjA5"}},{"cell_type":"code","source":["def calculate_federer_murray_ci(model_result, player_map):\n","    p1 = \"Federer R.\"\n","    p2 = \"Murray A.\"\n","\n","    # Check if players exist in the map\n","    if p1 not in player_map or p2 not in player_map:\n","        print(f\"Error: Could not find {p1} or {p2} in the model.\")\n","        return\n","\n","    # Retrieve Coefficients\n","    idx_p1 = player_map[p1]\n","    idx_p2 = player_map[p2]\n","    beta_p1 = model_result.params[idx_p1]\n","    beta_p2 = model_result.params[idx_p2]\n","\n","    # Linear predictor (eta)\n","    eta = beta_p1 - beta_p2\n","\n","    # Calculate Variance and Standard Error\n","    cov_matrix = model_result.cov_params()\n","\n","    var_p1 = cov_matrix[idx_p1, idx_p1]\n","    var_p2 = cov_matrix[idx_p2, idx_p2]\n","    cov_p1_p2 = cov_matrix[idx_p1, idx_p2]\n","\n","    # Var(A - B) = Var(A) + Var(B) - 2*Cov(A,B)\n","    var_diff = var_p1 + var_p2 - 2 * cov_p1_p2\n","    se_diff = np.sqrt(var_diff)\n","\n","    # Calculate 68% CI for the linear predictor (Z approx 1.0)\n","    z_score = 1.0\n","\n","    eta_lower = eta - (z_score * se_diff)\n","    eta_upper = eta + (z_score * se_diff)\n","\n","    # Transform using the logistic function\n","    def logistic(x):\n","        return 1.0 / (1.0 + np.exp(-x))\n","\n","    prob_est = logistic(eta)\n","    prob_lower = logistic(eta_lower)\n","    prob_upper = logistic(eta_upper)\n","\n","    return eta, prob_est, prob_lower, prob_upper\n","\n","eta, prob_est, prob_lower, prob_upper = calculate_federer_murray_ci(model_result, player_map)\n","print(f\"Probability Federer beats Murray: {prob_est:.4f}\")\n","print(f\"68% Confidence Interval: [{prob_lower:.4f}, {prob_upper:.4f}]\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8nB4s7Qz8JA","executionInfo":{"status":"ok","timestamp":1766625751218,"user_tz":0,"elapsed":47,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"230044e8-ce06-4f38-a9e1-53d99e2af69d"},"execution_count":182,"outputs":[{"output_type":"stream","name":"stdout","text":["Probability Federer beats Murray: 0.6526\n","68% Confidence Interval: [0.6122, 0.6910]\n"]}]},{"cell_type":"markdown","source":["It is well-known that certain players enjoy an advantage on specific\n","surfaces. We suggest a new model with\n","\\begin{equation}\n","    \\Pr(\\text{Player $a$ wins a match against Player $b$ on surface $s$}) = \\frac{\\exp(\\beta_a + \\beta_{a,s} - \\beta_b - \\beta_{b,s})}{1 + \\exp(\\beta_a + \\beta_{a,s} - \\beta_b - \\beta_{b,s})},\n","\\end{equation}\n","where $\\beta_{a,s}$ can be interpreted as the advantage of Player $a$ on surface $s$ compared to a baseline fitness $\\beta_a$."],"metadata":{"id":"Tlgbahqe1CPo"}},{"cell_type":"code","source":["def build_surface_design_matrix(data, feature_map, ref_player, ref_surface):\n","    N = len(data)\n","    P = len(feature_map)\n","    X = np.zeros((N, P))\n","\n","    winners = data['Winner'].values\n","    losers = data['Loser'].values\n","    surf = data['Surface'].values\n","\n","    for i in range(N):\n","        w, l, s = winners[i], losers[i], surf[i]\n","\n","        # Baseline Part (Beta_w - Beta_l)\n","        if w != ref_player and w in feature_map:\n","            X[i, feature_map[w]] = 1\n","        if l != ref_player and l in feature_map:\n","            X[i, feature_map[l]] = -1\n","\n","        # Interaction Part (Beta_w,s - Beta_l,s)\n","        if s != ref_surface:\n","            w_int = f\"{w}_{s}\"\n","            l_int = f\"{l}_{s}\"\n","\n","            # We must check if the interaction key exists\n","            if w_int in feature_map:\n","                X[i, feature_map[w_int]] = 1\n","            if l_int in feature_map:\n","                X[i, feature_map[l_int]] = -1\n","\n","    return X, np.ones(N)"],"metadata":{"id":"qMU35o011-3_","executionInfo":{"status":"ok","timestamp":1766625751254,"user_tz":0,"elapsed":20,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}}},"execution_count":183,"outputs":[]},{"cell_type":"code","source":["REF_PLAYER = \"Agassi A.\"\n","REF_SURFACE = \"Hard\"\n","\n","# Identify all players and surfaces from training data\n","players = sorted(list(set(df_train['Winner']) | set(df_train['Loser'])))\n","surfaces = sorted(list(df_train['Surface'].unique()))\n","# Baseline: All players except Agassi\n","baseline_vars = [p for p in players if p != REF_PLAYER]\n","# Interactions: All players (including Agassi) on non-Hard surfaces\n","interaction_vars = []\n","for p in players:\n","    for s in surfaces:\n","        if s != REF_SURFACE:\n","            interaction_vars.append(f\"{p}_{s}\")\n","\n","all_feature_names = baseline_vars + interaction_vars\n","feature_map = {name: i for i, name in enumerate(all_feature_names)}\n","\n","X_train_full, y_train = build_surface_design_matrix(df_train, feature_map, REF_PLAYER, REF_SURFACE)\n","# Identify columns that are all zeros (unused interactions)\n","active_cols_mask = np.abs(X_train_full).sum(axis=0) > 0\n","X_train_active = X_train_full[:, active_cols_mask]\n","\n","print(f\"Original features: {X_train_full.shape[1]}\")\n","print(f\"Active features:   {np.sum(active_cols_mask)}\")\n","\n","model_surf_result = train_bradley_terry(X_train_active, y_train)\n","X_test_full, y_test = build_surface_design_matrix(df_test, feature_map, REF_PLAYER, REF_SURFACE)\n","X_test_active = X_test_full[:, active_cols_mask]\n","\n","train_loss = calculate_logistic_loss(model_surf_result, X_train_active, y_train)\n","test_loss = calculate_logistic_loss(model_surf_result, X_test_active, y_test)\n","\n","print(f\"\\nSurface Model Train Loss: {train_loss:.5f}\")\n","print(f\"Surface Model Test Loss:  {test_loss:.5f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAxRR4Bm7zko","executionInfo":{"status":"ok","timestamp":1766625753290,"user_tz":0,"elapsed":2031,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"acd69451-e2e2-4bda-80f8-76a16f579c33"},"execution_count":184,"outputs":[{"output_type":"stream","name":"stdout","text":["Original features: 239\n","Active features:   233\n","\n","Surface Model Train Loss: 0.56377\n","Surface Model Test Loss:  0.65420\n"]}]},{"cell_type":"markdown","source":["The surface model has a lower training loss than the simple model. By adding surface-specific parameters, increasing model complexity, the model can fit the historical data much more closely. However, in the test data, the surface model performs worse than the simple model. This may be due to overfitting which is not allowing the model to generalise to new data.\n","\n","To formally compare the nested models where the simple model is a restricted version of the surface model, we use the likelihood ratio test.\n","\n","The null Hypothesis is $H_0$: The additional parameters are zero and the alternative Hypothes is $H_1$: At least one surface interaction parameter is non-zero. Our test statistic is\n","\\begin{equation}\n","    D = -2 \\log\\left( \\frac{\\mathcal{L}_{simple}}{\\mathcal{L}_{surface}} \\right)  = 2(\\ell_{surface} - \\ell_{simple})\n","\\end{equation}\n","where $\\ell$ is the log-likelihood of the fitted model. Under $H_0$, this statistic $D$ follows a Chi-squared distribution with degrees of freedom equal to the difference in the number of parameters between the two models.\n","\n","If we reject $H_0$, then it means that the improvement in training fit is statistically significant and not just due to random chance from adding more parameters. If the test loss for the surface model is lower than the simple model, then the hypothesis test agrees with the cross-validation/test-set results.\n","\n"],"metadata":{"id":"cKejbhPQ8WwZ"}},{"cell_type":"code","source":["from scipy.stats import chi2\n","\n","def perform_likelihood_ratio_test(model_simple, model_complex):\n","    # Retrieve Log-Likelihoods\n","    ll_simple = model_simple.llf\n","    ll_complex = model_complex.llf\n","\n","    # Calculate Test Statistic (Deviance)\n","    deviance = 2 * (ll_complex - ll_simple)\n","\n","    # Calculate Degrees of Freedom Difference\n","    df_diff = model_complex.df_model - model_simple.df_model\n","\n","    # Calculate p-value (Survival function of Chi-Squared distribution)\n","    p_value = chi2.sf(deviance, df_diff)\n","\n","    print(f\"Likelihood Ratio Test Results:\")\n","    print(f\"Simple Model LL:  {ll_simple:.4f}\")\n","    print(f\"Surface Model LL: {ll_complex:.4f}\")\n","    print(f\"Statistic (D):    {deviance:.4f}\")\n","    print(f\"df difference:    {int(df_diff)}\")\n","    print(f\"p-value:          {p_value:.4e}\\n\")\n","\n","    alpha = 0.01\n","    if p_value < alpha:\n","        print(f\"Reject the null hypothesis at {alpha*100}% level.\")\n","        print(\"The Surface Model is statistically significantly better.\")\n","    else:\n","        print(f\"Fail to reject the null hypothesis at {alpha*100}% level.\")\n","        print(\"The Simple Model is sufficient.\")\n","\n","perform_likelihood_ratio_test(model_result, model_surf_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rl7b_U219_WJ","executionInfo":{"status":"ok","timestamp":1766625753305,"user_tz":0,"elapsed":17,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"a611d5f8-c14a-4c3d-83a3-dbda2b15efe1"},"execution_count":185,"outputs":[{"output_type":"stream","name":"stdout","text":["Likelihood Ratio Test Results:\n","Simple Model LL:  -2690.5587\n","Surface Model LL: -2530.7623\n","Statistic (D):    319.5929\n","df difference:    171\n","p-value:          4.3545e-11\n","\n","Reject the null hypothesis at 1.0% level.\n","The Surface Model is statistically significantly better.\n"]}]},{"cell_type":"markdown","source":["Currently, our response variable is binary (Win/Loss). This ignores how close the match was. We can use the game counts W1-W5 and L1-L5 to create a more granular model. We sum the games won by the winner and the loser to get the total score\n","\\begin{align}\n","    y_i = \\sum_{k=1}^5 W_k, \\quad \\text{Total games won by the match winner}, \\\\\n","    n_i = \\sum_{k=1}^5 (W_k + L_k), \\quad \\text{Total games played in the match}.\n","\\end{align}\n","Instead of a Bernoulli distribution, we now use a binomial distribution to model the proportion of games won. We have $Y_i \\sim \\text{Binomial}(n_i, \\pi_i)$ so the link function is the Logit function\n","\\begin{equation}\n","    \\eta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right),\n","\\end{equation}\n","where the linear predictor is $\\eta = \\beta_a - \\beta_b$. Then $\\pi_i$ is the probability that Player A wins an individual game against Player B. The coefficients $\\beta$ now represent a player's ability to win games rather than matches. This approach effectively increases the sample size as each game contributes to the likelihood, allowing the model to distinguish between close or one-sided matches.\n","\n","The design matrix $X$ remains exactly the same as before. The response vector is altered so that instead of passing a vector of $1$'s, we instead pass the count of successes $y_i$ and trials $n_i$ to the GLM."],"metadata":{"id":"tX-sKwNs9-T-"}}]}
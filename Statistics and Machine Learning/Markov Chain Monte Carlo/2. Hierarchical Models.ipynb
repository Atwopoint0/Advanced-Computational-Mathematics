{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3eNFOYm9IHUdoWCKPN/ye"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Data from the performance of K football teams, over $T$ years has been scored on a scale of $0$ (no wins) to $114$ (win in all 38 games), with a win scoring three and a draw scoring one point.\n","\n","Model $Y_{kt}$, the score of the $k$-th team in year $t$, as\n","\\begin{equation}\n","    Y_{kt} | \\text{ parameters} \\sim N(\\mu_k, \\sigma_k^2) \\quad \\text{for} \\quad k = 1, \\dots, K \\quad \\text{and} \\quad t = 1, \\dots, T,\n","\\end{equation}\n","with the hierarchical prior structure that the team mean $\\mu_k$ and variance $\\sigma_k^2$ are independently distributed, given $\\theta$, as\n","\\begin{align}\n","    \\mu_k | \\theta &\\sim N(\\theta, \\sigma_0^2) \\\\\n","    \\sigma_k^{-2} &\\sim \\Gamma(\\alpha_0, \\beta_0),\n","\\end{align}\n","where $\\sigma_0^2$, $\\alpha_0$ and $\\beta_0$ are known parameters, and $\\theta$ is a second-stage prior with distribution\n","\\begin{equation}\n","    \\theta \\sim N(\\mu_0, \\tau_0^2),\n","\\end{equation}\n","where $\\mu_0$ and $\\tau_0^2$ are known parameters.\n","\n","The Gibbs sampler is well suited to the analysis of hierarchical models, since the full one-dimensional conditional distributions often have extremely simple forms. In the above model,\n","\\begin{align}\n","    \\mu_k | \\mu_{−k}, \\theta,\\sigma^2, y &\\sim N\\left(\\frac{\\sigma_k^{-2} \\sum_{t=1}^T y_{kt} + \\theta\\sigma_0^{-2}}{T \\sigma_k^{-2} + \\sigma_0^{-2}}, \\frac{1}{T \\sigma_k^{-2} + \\sigma_0^{-2}}\\right), \\\\\n","    \\theta | \\sigma^2, \\mu, y &\\sim N\\left(\\frac{\\sigma_0^{-2} \\sum_{k=1}^K \\mu_k + \\mu_0\\tau_0^{-2}}{K\\sigma_0^{-2} + \\tau_0^{-2}}, \\frac{1}{K\\sigma_0^{-2} + \\tau_0^{-2}}\\right), \\\\\n","    \\sigma_k^{-2} | \\sigma_{-k}^2, \\mu, \\theta, y &\\sim \\Gamma \\left(\\alpha_0 + \\frac{T}{2}, \\beta_0 + \\frac{1}{2}\\sum_{t=1}^T (y_{kt} − \\mu_k)^2\\right).\n","\\end{align}\n","\n","---\n","\n","Recall that the joint posterior is proportional to\n","\\begin{equation}\n","    \\Pr(\\mu, \\theta, \\sigma^2 | Y) \\propto \\left[ \\prod_{k=1}^K \\prod_{t=1}^T N(y_{kt} | \\mu_k, \\sigma_k^2) \\right] \\left[ \\prod_{k=1}^K N(\\mu_k | \\theta, \\sigma_0^2) \\right] N(\\theta | \\mu_0, \\tau_0^2) \\left[ \\prod_{k=1}^K \\Gamma(\\sigma_k^{-2} | \\alpha_0, \\beta_0) \\right].\n","\\end{equation}\n","\n","We seek $p(\\mu_k | \\mu_{-k}, \\theta, \\sigma^2, y)$. We isolate terms involving $\\mu_k$. The likelihood is\n","\\begin{equation}\n","    \\prod_{t=1}^T \\exp\\left( -\\frac{(y_{kt} - \\mu_k)^2}{2\\sigma_k^2} \\right) = \\exp\\left( -\\frac{\\sum_{t=1}^T (y_{kt} - \\mu_k)^2}{2\\sigma_k^2} \\right),\n","\\end{equation}\n","and the prior is\n","\\begin{equation}\n","    \\exp\\left( -\\frac{(\\mu_k - \\theta)^2}{2\\sigma_0^2} \\right).\n","\\end{equation}\n","Multiplying these gives the kernel of the conditional density\n","\\begin{equation}\n","    \\Pr(\\mu_k | \\dots) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{\\sum (y_{kt} - \\mu_k)^2}{\\sigma_k^2} + \\frac{(\\mu_k - \\theta)^2}{\\sigma_0^2} \\right] \\right).\n","\\end{equation}\n","This is a quadratic in $\\mu_k$ in the exponent, which implies a normal distribution. Examining the coefficients of $\\mu_k^2$ and $\\mu_k$, we see that the coefficient of $\\mu_k^2$ is $\\frac{T}{\\sigma_k^2} + \\frac{1}{\\sigma_0^2}$, so the variance is\n","\\begin{equation}\n","    \\left( \\frac{T}{\\sigma_k^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1} = \\left( T\\sigma_k^{-2} + \\sigma_0^{-2} \\right)^{-1},\n","\\end{equation}\n","while the mean is the weighted average of the data mean and the prior mean, weighted by their precisions,\n","\\begin{equation}\n","    \\frac{\\frac{\\sum y_{kt}}{\\sigma_k^2} + \\frac{\\theta}{\\sigma_0^2}}{\\frac{T}{\\sigma_k^2} + \\frac{1}{\\sigma_0^2}} = \\frac{\\sigma_k^{-2}\\sum_{t=1}^T y_{kt} + \\theta\\sigma_0^{-2}}{T\\sigma_k^{-2} + \\sigma_0^{-2}}.\n","\\end{equation}\n","\n","---\n","\n","We now seek $\\Pr(\\theta | \\sigma^2, \\mu, y)$ where we isolate terms involving $\\theta$. Similarly, the likelihood for $\\theta$ is\n","\\begin{equation}\n","    \\prod_{k=1}^K \\exp\\left( -\\frac{(\\mu_k - \\theta)^2}{2\\sigma_0^2} \\right) = \\exp\\left( -\\frac{\\sum_{k=1}^K (\\mu_k - \\theta)^2}{2\\sigma_0^2} \\right)\n","\\end{equation}\n","and the prior for $\\theta$ is\n","\\begin{equation}\n","    \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2} \\right).\n","\\end{equation}\n","Multiplying these gives\n","\\begin{equation}\n","    \\Pr(\\theta | \\dots) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{\\sum_{k=1}^K (\\mu_k - \\theta)^2}{\\sigma_0^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] \\right)\n","\\end{equation}\n","\n","This acts like a Bayesian update where we have $K$ observations of value $\\mu_k$ with variance $\\sigma_0^2$. Thus, the precision is\n","\\begin{equation}\n","    \\frac{K}{\\sigma_0^2} + \\frac{1}{\\tau_0^2} = K\\sigma_0^{-2} + \\tau_0^{-2},\n","\\end{equation}\n","and the mean is\n","\\begin{equation}\n","    \\frac{\\frac{\\sum \\mu_k}{\\sigma_0^2} + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{K}{\\sigma_0^2} + \\frac{1}{\\tau_0^2}} = \\frac{\\sigma_0^{-2}\\sum_{k=1}^K \\mu_k + \\mu_0\\tau_0^{-2}}{K\\sigma_0^{-2} + \\tau_0^{-2}}.\n","\\end{equation}\n","\n","---\n","\n","Let $\\tau_k = \\sigma_k^{-2}$. We isolate terms involving $\\sigma_k$ or $\\tau_k$. The prior is\n","\\begin{equation}\n","    \\tau_k \\sim \\Gamma(\\alpha_0, \\beta_0),\n","\\end{equation}\n","so the density is proportional to $\\tau_k^{\\alpha_0 - 1} e^{-\\beta_0 \\tau_k}$, and the likelihood is\n","\\begin{equation}\n","    \\prod_{t=1}^T \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp\\left( -\\frac{(y_{kt} - \\mu_k)^2}{2\\sigma_k^2} \\right),\n","\\end{equation}\n","where substituting $\\tau_k$, this is proportional to\n","\\begin{equation}\n","    (\\tau_k^{1/2})^T \\exp\\left( -\\frac{\\tau_k}{2} \\sum_{t=1}^T (y_{kt} - \\mu_k)^2 \\right) = \\tau_k^{T/2} \\exp\\left( -\\frac{\\tau_k}{2} \\sum (y_{kt} - \\mu_k)^2 \\right).\n","\\end{equation}\n","Combining the prior and likelihood, we obtain\n","\\begin{equation}\n","    \\Pr(\\tau_k | \\dots) \\propto \\tau_k^{(\\alpha_0 + T/2) - 1} \\exp\\left( -\\tau_k \\left[ \\beta_0 + \\frac{1}{2} \\sum_{t=1}^T (y_{kt} - \\mu_k)^2 \\right] \\right).\n","\\end{equation}\n","This is the kernel of a Gamma distribution $\\Gamma(\\alpha, \\beta)$ where the parameters are $\\alpha = \\alpha_0 + \\frac{T}{2}$ and $\\beta = \\beta_0 + \\frac{1}{2}\\sum_{t=1}^T (y_{kt} - \\mu_k)^2$."],"metadata":{"id":"-_AH0qFTyFyC"}},{"cell_type":"markdown","source":["---\n","\n","We want to find the distribution of $\\mu_k$ unconditional on $\\theta$, but given the parameters $\\mu_0, \\tau_0^2, \\sigma_0^2$. From the model definition, $\\theta \\sim N(\\mu_0, \\tau_0^2)$ and $\\mu_k | \\theta \\sim N(\\theta, \\sigma_0^2)$. We can express this relationship as a sum of independent random variables. Let $\\theta = \\mu_0 + \\epsilon_1$, where $\\epsilon_1 \\sim N(0, \\tau_0^2)$ and $\\mu_k = \\theta + \\epsilon_2$, where $\\epsilon_2 \\sim N(0, \\sigma_0^2)$. Then\n","\\begin{equation}\n","    \\mu_k = (\\mu_0 + \\epsilon_1) + \\epsilon_2 = \\mu_0 + (\\epsilon_1 + \\epsilon_2).\n","\\end{equation}\n","Since $\\epsilon_1$ and $\\epsilon_2$ are independent normal variables, their sum is also normal so the marginal prior distribution is\n","\\begin{equation}\n","    \\mu_k \\sim N(\\mu_0, \\tau_0^2 + \\sigma_0^2).\n","\\end{equation}"],"metadata":{"id":"1urgotWu7Lvb"}}]}
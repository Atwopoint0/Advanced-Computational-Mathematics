{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKxTxEpbji8AuS6WuOmiA9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Importance sampling is a technique for simulating random variables. Suppose that $X$ is a random variable with density $\\pi$, and suppose we wish to use simulation to estimate the probability that $X$ takes a value in some set $A$. If $X$ is difficult to simulate, or if the event that $X$ is in $A$ is very rare, then this might be hard to do.\n","\n","Suppose that we have some other random variable $Y$ with density $\\pi'$, such that $\\pi'(x) > 0$ whenever $\\pi(x) > 0$. Let $L$ be the likelihood ratio,\n","\\begin{equation}\n","    L(y) = \\frac{\\pi(y)}{\\pi'(y)}.\n","\\end{equation}\n","Let $\\gamma = \\Pr(X \\in A)$, and consider the estimator\n","\\begin{equation}\n","    \\hat{\\gamma} = L(Y)\\mathbb{1}[Y \\in A]\n","\\end{equation}\n","where $\\mathbb{1}[\\cdot]$ is the indicator function. We call $\\pi'$ the twisted density.\n","\n","We claim that $\\hat{\\gamma}$ is an unbiased estimator of $\\gamma$.\n","\n","\n","To find the expected value of $\\hat{\\gamma}$, we integrate the estimator with respect to the probability density function of $Y$,\n","\\begin{equation}\n","    E[\\hat{\\gamma}] = E[L(Y)\\mathbb{1}[Y \\in A]] = \\int_A L(y)\\pi'(y) \\,dy.\n","\\end{equation}\n","Now, substitute the definition of the likelihood ratio $L(y)$,\n","\\begin{equation}\n","    E[\\hat{\\gamma}] = \\int_A \\frac{\\pi(y)}{\\pi'(y)}\\pi'(y) \\,dy \\int_A = \\pi(y) \\,dy.\n","\\end{equation}\n","By definition, the integral of the probability density function $\\pi$ over the set $A$ is the probability that the random variable $X$ takes a value in $A$, which is precisely equal to $\\gamma$. Thus, we have shown that $E[\\hat{\\gamma}] = \\gamma$.\n","\n","Based on the fact that $\\hat{\\gamma}$ is an unbiased estimator of $\\gamma$, we can use a Monte Carlo simulation method to estimate $\\gamma$. The law of large numbers states that the average of a large number of i.i.d. samples of a random variable converges to its expected value. The proceedure is as follows:\n","\n","*   Draw a large number $n$ of independent samples $Y_1, \\dots, Y_n$ from the importance distribution with density $\\pi'(y)$.\n","*   For each sample $Y_i$, calculate a value for the estimator $\\hat{\\gamma}_i$:\n","    *   Compute the likelihood ratio $L(Y_i) = \\pi(Y_i)/\\pi'(Y_i)$;\n","    *   Check if the sample falls within the set $A$;\n","    *   Calculate the estimate $\\hat{\\gamma}_i = L(Y_i) \\mathbb{1}[Y_i \\in A]$. This will be $L(Y_i)$ if $Y_i \\in A$ and $0$ otherwise.\n","*   The estimate of $\\gamma$ is the sample mean of all the individual estimates $\\hat{\\gamma}_i$\n","\\begin{equation}\n","    \\hat{\\gamma} = \\frac{1}{n} \\sum_{i = 1}^n \\hat{\\gamma}_i = \\frac{1}{n} \\sum_{i=1}^n L(Y_i) \\mathbb{1}[Y_i \\in A].\n","\\end{equation}\n","As the number of samples $n$ increases, this estimated value will converge to the true value of $\\gamma$.\n","\n","This method is called importance sampling."],"metadata":{"id":"xTPEEOgE88Bf"}},{"cell_type":"markdown","source":["Now let $X$ be an exponential random variable with mean $3$, and consider the event $B = \\{X > 30\\}$. Suppose we wish to estimate $\\Pr(B)$ by importance sampling, using as our twisted distribution an exponential with mean $\\lambda^{-1}$.\n","\n","We first calculate the exact probability of the event $B$. The probability density function (PDF) of an exponential distribution with mean $3$\n","\\begin{equation}\n","    \\pi(x) = \\frac{1}{3} e^{-x/3}.\n","\\end{equation}\n","The probability of $B$, is calculated by integrating the PDF,\n","\\begin{equation}\n","    \\Pr(B) = \\int_{30}^\\infty \\frac{1}{3} e^{-x/3} \\,dx = [-e^{-x/3}]_{30}^\\infty = e^{-10} \\approx 4.54 \\cdot 10^{-5}.\n","\\end{equation}\n","This is a very small probability, which makes it a good candidate for estimation using importance sampling, as standard Monte Carlo methods would require an extremely large number of samples to observe the event."],"metadata":{"id":"PXH9tjcvDkbU"}},{"cell_type":"code","source":["import numpy as np\n","\n","def importance_sampling_estimator(lambda_val, num_samples=1000000):\n","    '''\n","    Estimates the probability P(X > 30) for an exponential random variable X\n","    with mean 3, using importance sampling.\n","    Args:\n","        lambda_val: The rate parameter for the twisted (proposal) exponential distribution.\n","        num_samples: The number of samples to generate for the estimation.\n","    Returns:\n","        A tuple containing the estimated probability and the variance of the estimates.\n","    '''\n","    # Generate uniform random variables to be transformed into exponential samples.\n","    uniform_samples = np.random.uniform(0, 1, num_samples)\n","    # Transform uniform samples to exponential samples.\n","    proposal_samples = -np.log(uniform_samples) / lambda_val\n","\n","    # Identify which samples fall into the event of interest (B = {Y > 30}).\n","    samples_in_event_B = proposal_samples[proposal_samples > 30]\n","    if len(samples_in_event_B) == 0:\n","        return 0.0, 0.0\n","\n","    # Calculate the likelihood ratio L(y) for the samples in B.\n","    # L(y) = (1 / (3 * lambda)) * exp(y * (lambda - 1/3))\n","    likelihood_ratios = (1 / (3 * lambda_val)) * np.exp(samples_in_event_B * (lambda_val - 1/3.0))\n","    estimated_prob = np.sum(likelihood_ratios) / num_samples\n","\n","    # Calculate the variance of the estimates.\n","    squared_estimates = likelihood_ratios**2\n","    variance = (np.sum(squared_estimates) / num_samples) - (estimated_prob**2)\n","    return estimated_prob, variance\n","\n","\n","original_lambda = 1/3.0\n","true_probability = np.exp(-10)\n","lambda_values_to_test = [1/2, 1/3, 1/10]\n","\n","print(f\"True Probability P(X > 30) = e^(-10) ≈ {true_probability:.10f}\")\n","print(\"-\" * 70)\n","print(f\"{'Lambda (λ)':<15} | {'Mean (1/λ)':<12} | {'Estimated P(B)':<20} | {'Variance':<15}\")\n","print(\"-\" * 70)\n","\n","for l_val in lambda_values_to_test:\n","    mean_val = 1/l_val\n","    est_prob, var = importance_sampling_estimator(l_val)\n","    print(f\"{l_val:<15.4f} | {mean_val:<12.2f} | {est_prob:<20.10f} | {var:<15.10f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKVC-BohFbuY","executionInfo":{"status":"ok","timestamp":1759630703637,"user_tz":-60,"elapsed":93,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"b2a23fcd-99c5-4693-9f2b-eeaa32593f2d"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["True Probability P(X > 30) = e^(-10) ≈ 0.0000453999\n","----------------------------------------------------------------------\n","Lambda (λ)      | Mean (1/λ)   | Estimated P(B)       | Variance       \n","----------------------------------------------------------------------\n","0.5000          | 2.00         | 0.0000000000         | 0.0000000000   \n","0.3333          | 3.00         | 0.0000530000         | 0.0000529972   \n","0.1000          | 10.00        | 0.0000457811         | 0.0000000797   \n"]}]},{"cell_type":"markdown","source":["*   $\\lambda = 1/2$: Here, the proposal distribution has a mean of $2$ which is smaller than the original mean of $3$. Consequently, the samples are concentrated far away from the region of interest $Y > 30$. In a typical run, it is highly probable that zero samples will be greater than $30$, leading to an estimated probability of $0$. This is a very poor choice for $\\lambda$.\n","\n","*   $\\lambda = 1/3$: This sets the proposal distribution to be the same as the original distribution and is equivalent to a standard Monte Carlo simulation. Because the event $\\{X > 30\\}$ is so rare, it is very likely that none or very few of the samples will fall into this region. The typical result is a very low estimate, close to $0$, which demonstrates the inefficiency of standard Monte Carlo for this problem.\n","\n","*   $\\lambda = 1/10$: The proposal distribution now has a mean of $10$. This is larger than the original mean, which means we are generating more samples in the tail of the distribution, closer to our region of interest. This choice of $\\lambda$ produces an estimate that is very close to the true value of $e^{-10}$. The variance is small, but non-zero, indicating a good estimator.\n","\n","---\n","\n","To determine how long a simulation is required we shall calculate the relative error, which measures the estimator's standard deviation as a fraction of the estimate itself.\n","\n","The standard deviation of our mean estimate is called the standard error, calculated as:\n","\\begin{equation}\n","    \\mathop{SE} = \\frac{\\sigma}{\\sqrt{n}},\n","\\end{equation}\n","where $\\sigma$ is the standard deviation of a single estimate $\\hat{\\gamma}_i$ and $n$ is the number of samples. The relative error is\n","\\begin{equation}\n","    \\mathop{RE} = \\frac{\\mathop{SE}}{|\\mu|} = \\frac{\\sigma}{|\\mu|\\sqrt{n}},\n","\\end{equation}\n","where $\\mu$ is the true value we are estimating. We can now set a target for our desired relative error and solve for the number of samples $n$ required to achieve it\n","\\begin{equation}\n","    n = \\left(\\frac{\\sigma}{|\\mu|\\mathop{RE}}\\right)² = \\frac{Var(\\hat{\\gamma})}{\\mu^2 \\mathop{RE}^2}.\n","\\end{equation}\n","Since we don't know the true variance or the true mean beforehand, we can run a smaller simulation to obtain sample estimates for them. We then use these estimates to project the total number of samples needed:\n","\\begin{equation}\n","    n \\approx \\frac{s^2}{\\bar{x}^2 \\mathop{RE}^2}.\n","\\end{equation}\n","\n","If the proposal mean $1/\\lambda$ is too small, then this has the same problem as the original distribution. We will very rarely generate a sample greater than $30$, making the simulation extremely inefficient. The variance is low, but the estimate itself is often zero, so we need an enormous number of samples to get a stable result.\n","\n","If the proposal mean $1/\\lambda$ is too large, then we will generate almost all of our samples in the region $X > 30$. The likelihood ratio must correct for the fact that we are using a different distribution so it can become very large and vary significantly, which leads to high variance in the estimates. A high variance means we need more samples to get a reliable average."],"metadata":{"id":"INVcAh2nGiKg"}},{"cell_type":"code","source":["import numpy as np\n","\n","def estimate_required_samples(lambda_val, pilot_samples=10000, target_rel_error=0.01):\n","    '''\n","    Estimates the number of samples required to achieve a target relative error\n","    for a given lambda.\n","    Args:\n","        lambda_val: The rate parameter for the proposal distribution.\n","        pilot_samples: The number of samples for the initial pilot run.\n","        target_rel_error: The desired relative error for the final estimate.\n","    Returns:\n","        The estimated number of samples required.\n","    '''\n","    uniform_samples = np.random.uniform(0, 1, pilot_samples)\n","    proposal_samples = -np.log(uniform_samples) / lambda_val\n","    all_estimates = np.zeros(pilot_samples)\n","\n","    in_event_indices = np.where(proposal_samples > 30)[0]\n","    samples_in_event_B = proposal_samples[in_event_indices]\n","    if len(samples_in_event_B) == 0:\n","        return float('inf')\n","\n","    likelihood_ratios = (1 / (3 * lambda_val)) * np.exp(samples_in_event_B * (lambda_val - 1/3.0))\n","    all_estimates[in_event_indices] = likelihood_ratios\n","    sample_mean = np.mean(all_estimates)\n","    sample_variance = np.var(all_estimates)\n","    if sample_mean < 1e-15:\n","        return float('inf')\n","\n","    required_n = sample_variance / (sample_mean**2 * target_rel_error**2)\n","    return required_n\n","\n","true_probability = np.exp(-10)\n","lambda_values = 1.0 / np.linspace(5, 50, 46)\n","\n","print(f\"Estimating required sample size for a 1% relative error.\")\n","print(\"-\" * 65)\n","print(f\"{'Lambda (λ)':<12} | {'Mean (1/λ)':<12} | {'Required Samples (n)':<25}\")\n","print(\"-\" * 65)\n","\n","best_lambda = -1\n","min_samples = float('inf')\n","\n","for l_val in lambda_values:\n","    required_n = estimate_required_samples(l_val)\n","    if required_n < min_samples:\n","        min_samples = required_n\n","        best_lambda = l_val\n","\n","    print(f\"{l_val:<12.4f} | {1/l_val:<12.2f} | {required_n:<25,.0f}\")\n","\n","print(\"-\" * 65)\n","print(f\"Optimal λ found: {best_lambda:.4f} (corresponding mean ≈ {1/best_lambda:.2f})\")\n","print(f\"Minimum estimated samples required: {min_samples:,.0f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEo3hYwLJCjE","executionInfo":{"status":"ok","timestamp":1759630703685,"user_tz":-60,"elapsed":42,"user":{"displayName":"Alex Zhou","userId":"15814347624889041767"}},"outputId":"bd4a3b70-8349-4cbb-ec7c-17165b6a65b5"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Estimating required sample size for a 1% relative error.\n","-----------------------------------------------------------------\n","Lambda (λ)   | Mean (1/λ)   | Required Samples (n)     \n","-----------------------------------------------------------------\n","0.2000       | 5.00         | 3,884,181                \n","0.1667       | 6.00         | 2,424,512                \n","0.1429       | 7.00         | 1,063,282                \n","0.1250       | 8.00         | 637,474                  \n","0.1111       | 9.00         | 474,211                  \n","0.1000       | 10.00        | 383,743                  \n","0.0909       | 11.00        | 299,042                  \n","0.0833       | 12.00        | 260,515                  \n","0.0769       | 13.00        | 246,905                  \n","0.0714       | 14.00        | 214,731                  \n","0.0667       | 15.00        | 200,165                  \n","0.0625       | 16.00        | 189,767                  \n","0.0588       | 17.00        | 170,321                  \n","0.0556       | 18.00        | 162,595                  \n","0.0526       | 19.00        | 161,243                  \n","0.0500       | 20.00        | 151,313                  \n","0.0476       | 21.00        | 144,351                  \n","0.0455       | 22.00        | 153,816                  \n","0.0435       | 23.00        | 138,230                  \n","0.0417       | 24.00        | 135,132                  \n","0.0400       | 25.00        | 135,810                  \n","0.0385       | 26.00        | 128,163                  \n","0.0370       | 27.00        | 141,052                  \n","0.0357       | 28.00        | 132,890                  \n","0.0345       | 29.00        | 127,227                  \n","0.0333       | 30.00        | 136,372                  \n","0.0323       | 31.00        | 138,288                  \n","0.0312       | 32.00        | 124,409                  \n","0.0303       | 33.00        | 134,495                  \n","0.0294       | 34.00        | 138,545                  \n","0.0286       | 35.00        | 135,626                  \n","0.0278       | 36.00        | 136,407                  \n","0.0270       | 37.00        | 133,890                  \n","0.0263       | 38.00        | 138,142                  \n","0.0256       | 39.00        | 133,867                  \n","0.0250       | 40.00        | 131,883                  \n","0.0244       | 41.00        | 141,649                  \n","0.0238       | 42.00        | 138,697                  \n","0.0233       | 43.00        | 145,341                  \n","0.0227       | 44.00        | 141,848                  \n","0.0222       | 45.00        | 137,510                  \n","0.0217       | 46.00        | 140,772                  \n","0.0213       | 47.00        | 140,293                  \n","0.0208       | 48.00        | 141,293                  \n","0.0204       | 49.00        | 136,935                  \n","0.0200       | 50.00        | 143,703                  \n","-----------------------------------------------------------------\n","Optimal λ found: 0.0312 (corresponding mean ≈ 32.00)\n","Minimum estimated samples required: 124,409\n"]}]},{"cell_type":"markdown","source":["A simulation or its resulting estimator is considered useless if its variance is infinite. An estimator with infinite variance does not converge reliably, and the standard error does not decrease as the sample size increases, violating the conditions in the central limit theorem.\n","\n","The variance of the importance sampling estimator $\\hat{\\gamma}$ is given by\n","\\begin{equation}\n","    Var(\\hat{\\gamma}) = E[\\hat{\\gamma}^2] - E[\\hat{\\gamma}]^2.\n","\\end{equation}\n","Since we already know  that $E[\\hat{\\gamma}] = \\gamma = e^{-10} < \\infty$, the variance is infinite if and only if the second moment, $E[\\hat{\\gamma}^2]$, is infinite. This is given by\n","\\begin{align}\n","    E[\\hat{\\gamma}^2]\n","    &= \\int_0^\\infty \\hat{\\gamma}(y)^2 \\pi'(y) \\,dy \\\\\n","    &= \\int_0^\\infty \\left(\\frac{\\pi(y)}{\\pi'(y)} \\mathbb{1}[y > 30]\\right)^2 \\pi'(y) \\,dy \\\\\n","    &= ∫_{30}^\\infty \\frac{\\pi(y)^2}{\\pi'(y)} \\,dy.\n","\\end{align}\n","Now, substitute the formulas for the probability density functions $\\pi(y) = e^{-y/3}/3$ and $\\pi'(y) = \\lambda e^{-\\lambda y}$ to get\n","\\begin{equation}\n","    E[\\hat{\\gamma}^2] = \\frac{1}{9\\lambda} \\int_{30}^\\infty e^{y(\\lambda - 2/3)} \\,dy.\n","\\end{equation}\n","This integral will converge if and only if  $\\lambda < 2/3$.\n","\n","The optimal value of $\\lambda$ is the one that minimises the variance of the estimator or equivalently, the second moment. Evaluating the above expression, we obtain\n","\\begin{equation}\n","    E[\\hat{\\gamma}^2] = \\frac{1}{9\\lambda} \\left[ \\frac{1}{\\lambda - 2/3} e^{y(\\lambda - 2/3)} \\right]_{30}^\\infty =  \\frac{1}{9\\lambda(2/3 - \\lambda)} e^{30(λ - 2/3)}.\n","\\end{equation}\n","Let $f(\\lambda) = E[\\hat{\\gamma}^2]$. Minimising $f(\\lambda)$ is equivalent to minimising\n","\\begin{equation}\n","    \\log(f(\\lambda)) = \\log(1) - \\log(9) - \\log(\\lambda) - \\log(2/3 - \\lambda) + 30(\\lambda - 2/3).\n","\\end{equation}\n","Now, differentiate with respect to $\\lambda$\n","\\begin{equation}\n","    \\frac{d}{d\\lambda} \\log(f(\\lambda)) = -\\frac{1}{\\lambda} + \\frac{1}{2/3 - λ} + 30\n","\\end{equation}\n","which has two possible solutions $\\lambda = (33 \\pm \\sqrt{909}) / 90$ of which only one falls inside the range of finite variance $\\lambda < 2/3$ which is given by $\\lambda = (33 - √909) / 90 \\approx 0.0317$."],"metadata":{"id":"rN1pGPAMKoUs"}}]}